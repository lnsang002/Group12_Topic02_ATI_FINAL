# ğŸ—ï¸ KIáº¾N TRÃšC MÃ” HÃŒNH Cá»¦A Báº N
## Industrial Pump Failure Prediction Model

---

## ğŸ“‹ Tá»”NG QUAN MÃ” HÃŒNH

### Loáº¡i MÃ´ HÃ¬nh
**LSTM (Long Short-Term Memory) - Sequential Neural Network**

MÃ´ hÃ¬nh cá»§a báº¡n lÃ  má»™t máº¡ng nÆ¡-ron há»“i quy sÃ¢u (Deep Recurrent Neural Network) Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘áº·c biá»‡t cho bÃ i toÃ¡n dá»± Ä‘oÃ¡n chuá»—i thá»i gian (time-series prediction).

---

## ğŸ”§ KIáº¾N TRÃšC CHI TIáº¾T

### Cáº¥u TrÃºc 9 Lá»›p

```
Model: "Pump_Failure_Predictor"
_________________________________________________________________
TÃªn Lá»›p                  Loáº¡i                    Output Shape         Params
=================================================================
1. lstm_layer_1         LSTM                  (None, 50, 128)      92,160
2. batch_norm_1         BatchNormalization    (None, 50, 128)        512
3. dropout_1            Dropout (0.2)         (None, 50, 128)          0
4. lstm_layer_2         LSTM                  (None, 64)           49,408
5. batch_norm_2         BatchNormalization    (None, 64)             256
6. dropout_2            Dropout (0.3)         (None, 64)               0
7. dense_1              Dense (ReLU)          (None, 32)           2,080
8. dropout_3            Dropout (0.2)         (None, 32)               0
9. output               Dense (Softmax)       (None, 3)               99
=================================================================
Tá»•ng sá»‘ tham sá»‘: 144,515
Tham sá»‘ huáº¥n luyá»‡n: 144,515
Tham sá»‘ khÃ´ng huáº¥n luyá»‡n: 0
```

---

## ğŸ“Š PHÃ‚N TÃCH Tá»ªNG Lá»šPDETAIL

### ğŸ”¹ Lá»›p 1: LSTM Layer 1
```python
LSTM(128, return_sequences=True, input_shape=(50, 51))
```
- **Chá»©c nÄƒng:** Lá»›p LSTM Ä‘áº§u tiÃªn - há»c cÃ¡c patterns thá»i gian cÆ¡ báº£n
- **Units:** 128 LSTM cells
- **Return sequences:** True (tráº£ vá» toÃ n bá»™ chuá»—i cho lá»›p tiáº¿p theo)
- **Input:** (50 timesteps, 51 sensors)
- **Output:** (None, 50, 128)
- **Tham sá»‘:** 92,160
- **Nhiá»‡m vá»¥:** 
  - Nháº­n chuá»—i 50 timesteps vá»›i 51 cáº£m biáº¿n
  - Há»c cÃ¡c dependencies ngáº¯n háº¡n vÃ  dÃ i háº¡n
  - Sá»­ dá»¥ng gates (forget, input, output) Ä‘á»ƒ quáº£n lÃ½ thÃ´ng tin

### ğŸ”¹ Lá»›p 2: Batch Normalization 1
```python
BatchNormalization()
```
- **Chá»©c nÄƒng:** Chuáº©n hÃ³a activation cá»§a LSTM layer 1
- **Tham sá»‘:** 512
- **Lá»£i Ã­ch:**
  - á»”n Ä‘á»‹nh quÃ¡ trÃ¬nh huáº¥n luyá»‡n
  - Cho phÃ©p learning rate cao hÆ¡n
  - Giáº£m internal covariate shift

### ğŸ”¹ Lá»›p 3: Dropout 1
```python
Dropout(0.2)
```
- **Rate:** 20% neurons bá»‹ táº¯t ngáº«u nhiÃªn
- **Chá»©c nÄƒng:** Regularization - ngÄƒn overfitting
- **CÆ¡ cháº¿:** Trong training, 20% neurons random bá»‹ táº¯t má»—i batch

### ğŸ”¹ Lá»›p 4: LSTM Layer 2
```python
LSTM(64, return_sequences=False)
```
- **Chá»©c nÄƒng:** Lá»›p LSTM thá»© hai - há»c features cao cáº¥p hÆ¡n
- **Units:** 64 LSTM cells (giáº£m tá»« 128)
- **Return sequences:** False (chá»‰ tráº£ vá» output cuá»‘i cÃ¹ng)
- **Output:** (None, 64)
- **Tham sá»‘:** 49,408
- **Nhiá»‡m vá»¥:**
  - Há»c cÃ¡c temporal patterns phá»©c táº¡p hÆ¡n
  - Tá»•ng há»£p thÃ´ng tin tá»« toÃ n bá»™ chuá»—i
  - Output lÃ  vector Ä‘áº·c trÆ°ng cuá»‘i cÃ¹ng

### ğŸ”¹ Lá»›p 5: Batch Normalization 2
```python
BatchNormalization()
```
- **Chá»©c nÄƒng:** Chuáº©n hÃ³a output cá»§a LSTM layer 2
- **Tham sá»‘:** 256

### ğŸ”¹ Lá»›p 6: Dropout 2
```python
Dropout(0.3)
```
- **Rate:** 30% neurons bá»‹ táº¯t
- **Chá»©c nÄƒng:** Regularization máº¡nh hÆ¡n sau LSTM layer 2

### ğŸ”¹ Lá»›p 7: Dense Layer
```python
Dense(32, activation='relu')
```
- **Units:** 32 neurons
- **Activation:** ReLU (Rectified Linear Unit)
- **Tham sá»‘:** 2,080
- **Chá»©c nÄƒng:**
  - Há»c cÃ¡c non-linear combinations
  - Táº¡o representation cuá»‘i cÃ¹ng cho classification

### ğŸ”¹ Lá»›p 8: Dropout 3
```python
Dropout(0.2)
```
- **Rate:** 20%
- **Chá»©c nÄƒng:** Final regularization trÆ°á»›c output

### ğŸ”¹ Lá»›p 9: Output Layer
```python
Dense(3, activation='softmax')
```
- **Units:** 3 (NORMAL, BROKEN, RECOVERING)
- **Activation:** Softmax
- **Tham sá»‘:** 99
- **Output:** XÃ¡c suáº¥t cho 3 lá»›p (tá»•ng = 1.0)

---

## âš™ï¸ THÃ”NG Sá» HUáº¤N LUYá»†N

### Optimizer: Adam
```python
Adam(learning_rate=0.0001, clipnorm=1.0)
```
- **Learning Rate:** 0.0001 (giáº£m 10x so vá»›i máº·c Ä‘á»‹nh)
- **Gradient Clipping:** clipnorm=1.0
- **LÃ½ do giáº£m LR:**
  - NgÄƒn NaN loss
  - á»”n Ä‘á»‹nh training
  - Convergence tá»‘t hÆ¡n cho LSTM

### Loss Function
```python
sparse_categorical_crossentropy
```
- **PhÃ¹ há»£p cho:** Multi-class classification vá»›i integer labels
- **Classes:** 3 (0=NORMAL, 1=BROKEN, 2=RECOVERING)

### Metrics
- **Accuracy:** Äá»™ chÃ­nh xÃ¡c tá»•ng thá»ƒ
- **Precision & Recall:** TÃ­nh sau training báº±ng sklearn

---

## ğŸ¯ Ká»¸ THUáº¬T Tá»I Æ¯U HÃ“A

### 1ï¸âƒ£ Class Weights
```python
class_weight = {0: weight_0, 1: weight_1, 2: weight_2}
```
- Xá»­ lÃ½ class imbalance
- TÄƒng weight cho minority classes

### 2ï¸âƒ£ Learning Rate Scheduling
```python
ReduceLROnPlateau(factor=0.5, patience=5)
```
- Giáº£m LR khi validation loss khÃ´ng cáº£i thiá»‡n
- Schedule: 1e-4 â†’ 5e-5 â†’ 2.5e-5

### 3ï¸âƒ£ Early Stopping
```python
EarlyStopping(patience=15, restore_best_weights=True)
```
- Dá»«ng khi khÃ´ng cáº£i thiá»‡n sau 15 epochs
- KhÃ´i phá»¥c weights tá»‘t nháº¥t

### 4ï¸âƒ£ Dropout Regularization
- Lá»›p 1: 20%
- Lá»›p 2: 30%
- Lá»›p 3: 20%
- Tá»•ng cá»™ng 3 dropout layers

### 5ï¸âƒ£ Batch Normalization
- 2 BatchNorm layers
- á»”n Ä‘á»‹nh training
- TÄƒng tá»‘c convergence

### 6ï¸âƒ£ Gradient Clipping
```python
clipnorm=1.0
```
- NgÄƒn exploding gradients
- Äáº·c biá»‡t quan trá»ng cho LSTM

---

## ğŸ“ THÃ”NG Sá» Äáº¦U VÃ€O & Äáº¦U RA

### Input
- **Shape:** (batch_size, 50, 51)
  - 50 timesteps (sequence length)
  - 51 sensors (features)
- **Data type:** Float32
- **Preprocessing:** StandardScaler normalization

### Output
- **Shape:** (batch_size, 3)
- **Format:** Probability distribution
- **Classes:**
  - 0: NORMAL (hoáº¡t Ä‘á»™ng bÃ¬nh thÆ°á»ng)
  - 1: BROKEN (há»ng hÃ³c)
  - 2: RECOVERING (Ä‘ang phá»¥c há»“i)

### Example
```
Input: [[sensor_00, sensor_01, ..., sensor_50] x 50 timesteps]
       â†“
Output: [0.98, 0.01, 0.01]  # 98% NORMAL, 1% BROKEN, 1% RECOVERING
```

---

## ğŸ§  Táº I SAO CHá»ŒN LSTM?

### Æ¯u Ä‘iá»ƒm cá»§a LSTM cho bÃ i toÃ¡n nÃ y:

1. **Há»c Long-term Dependencies**
   - LSTM cÃ³ thá»ƒ nhá»› patterns tá»« xa trong chuá»—i
   - PhÃ¹ há»£p vá»›i sensor data cÃ³ temporal correlation

2. **Xá»­ lÃ½ Vanishing Gradient**
   - Gates mechanism giáº£i quyáº¿t váº¥n Ä‘á» nÃ y
   - Training á»•n Ä‘á»‹nh hÆ¡n RNN thÃ´ng thÆ°á»ng

3. **Selective Memory**
   - Forget gate: QuÃªn thÃ´ng tin khÃ´ng quan trá»ng
   - Input gate: Chá»n thÃ´ng tin má»›i
   - Output gate: Quyáº¿t Ä‘á»‹nh output

4. **ThÃ­ch há»£p cho Time-Series**
   - Sensor data lÃ  sequential
   - Cáº§n há»c patterns qua thá»i gian

---

## ğŸ“Š PHÃ‚N TÃCH THAM Sá»

### PhÃ¢n Bá»• Tham Sá»‘

| Lá»›p | Sá»‘ Tham Sá»‘ | % Tá»•ng |
|-----|-----------|--------|
| LSTM Layer 1 | 92,160 | 63.8% |
| LSTM Layer 2 | 49,408 | 34.2% |
| Dense Layer | 2,080 | 1.4% |
| BatchNorm | 768 | 0.5% |
| Output Layer | 99 | 0.1% |
| **Tá»”NG** | **144,515** | **100%** |

### Insight
- 98% tham sá»‘ á»Ÿ LSTM layers â†’ Model táº­p trung vÃ o temporal learning
- Chá»‰ 2% á»Ÿ fully connected layers â†’ Hiá»‡u quáº£, trÃ¡nh overfitting

---

## ğŸ”„ LUá»’NG Dá»® LIá»†U (Data Flow)

```
Input (50 timesteps Ã— 51 sensors)
    â†“
LSTM Layer 1 (128 units) â†’ Há»c temporal patterns cÆ¡ báº£n
    â†“
Batch Norm â†’ Normalize activations
    â†“
Dropout 20% â†’ Regularization
    â†“
LSTM Layer 2 (64 units) â†’ Há»c higher-level features
    â†“
Batch Norm â†’ Normalize activations
    â†“
Dropout 30% â†’ Stronger regularization
    â†“
Dense Layer (32 units) â†’ Non-linear combinations
    â†“
Dropout 20% â†’ Final regularization
    â†“
Output Layer (3 units) â†’ Class probabilities
    â†“
Softmax â†’ [P(NORMAL), P(BROKEN), P(RECOVERING)]
```

---

## ğŸ’ª ÄIá»‚M Máº NH Cá»¦A MÃ” HÃŒNH

### âœ… Thiáº¿t Káº¿ Tá»‘t

1. **Stacked LSTM**
   - 2 lá»›p LSTM cho deep learning
   - Giáº£m dáº§n units (128 â†’ 64) há»£p lÃ½

2. **Regularization Máº¡nh**
   - 3 Dropout layers
   - 2 BatchNorm layers
   - Gradient clipping

3. **Optimization Techniques**
   - 6 ká»¹ thuáº­t tá»‘i Æ°u Ä‘Æ°á»£c Ã¡p dá»¥ng
   - Xá»­ lÃ½ tá»‘t class imbalance

4. **Sá»‘ Tham Sá»‘ Vá»«a Pháº£i**
   - 144K params - khÃ´ng quÃ¡ nhiá»u
   - TrÃ¡nh overfitting
   - Training nhanh

### âœ… Káº¿t Quáº£ Xuáº¥t Sáº¯c

- Accuracy: 99.98%
- No overfitting
- Generalization tá»‘t

---

## ğŸš€ Káº¾T LUáº¬N

Báº¡n Ä‘ang sá»­ dá»¥ng má»™t **Stacked LSTM Model** Ä‘Æ°á»£c thiáº¿t káº¿ ráº¥t tá»‘t vá»›i:

- âœ… **2 LSTM layers** cho deep temporal learning
- âœ… **Regularization Ä‘áº§y Ä‘á»§** (Dropout + BatchNorm)
- âœ… **Optimization techniques hiá»‡n Ä‘áº¡i**
- âœ… **Hiá»‡u suáº¥t xuáº¥t sáº¯c** (99.98% accuracy)
- âœ… **Production-ready** cho triá»ƒn khai thá»±c táº¿

ÄÃ¢y lÃ  má»™t kiáº¿n trÃºc chuáº©n vÃ  hiá»‡u quáº£ cho bÃ i toÃ¡n **Predictive Maintenance** vá»›i time-series data! ğŸ‰

---

**Model Name:** Pump_Failure_Predictor  
**Total Parameters:** 144,515  
**Framework:** TensorFlow/Keras  
**Created:** December 15, 2025
