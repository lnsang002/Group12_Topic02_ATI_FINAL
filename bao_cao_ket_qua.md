# ğŸ“Š BÃO CÃO PHÃ‚N TÃCH Káº¾T QUáº¢ HUáº¤N LUYá»†N
## Dá»± Ä‘oÃ¡n Há»ng hÃ³c MÃ¡y bÆ¡m CÃ´ng nghiá»‡p - 62FIT4ATI

---

## ğŸ¯ Tá»”NG QUAN Dá»° ÃN

### Má»¥c tiÃªu
XÃ¢y dá»±ng mÃ´ hÃ¬nh máº¡ng nÆ¡-ron há»“i quy LSTM Ä‘á»ƒ dá»± Ä‘oÃ¡n tÃ¬nh tráº¡ng há»ng hÃ³c cá»§a mÃ¡y bÆ¡m cÃ´ng nghiá»‡p dá»±a trÃªn dá»¯ liá»‡u cáº£m biáº¿n chuá»—i thá»i gian.

### ThÃ´ng tin Dataset
- **Tá»•ng sá»‘ máº«u:** 220,320 Ä‘iá»ƒm dá»¯ liá»‡u
- **Sá»‘ lÆ°á»£ng cáº£m biáº¿n:** 52 cáº£m biáº¿n liÃªn tá»¥c
- **Sá»‘ lÆ°á»£ng chuá»—i:** 22,027 chuá»—i thá»i gian
- **PhÃ¢n loáº¡i:** 3 lá»›p (NORMAL, BROKEN, RECOVERING)
- **ThÃ¡ch thá»©c:** Máº¥t cÃ¢n báº±ng lá»›p nghiÃªm trá»ng

### Kiáº¿n trÃºc MÃ´ hÃ¬nh
- **Loáº¡i:** LSTM (Long Short-Term Memory)
- **Tá»•ng sá»‘ tham sá»‘:** 144,515 tham sá»‘
- **Sá»‘ lá»›p:** 9 lá»›p
- **Äá»™ dÃ i chuá»—i:** 50 timesteps
- **Äáº§u vÃ o:** 51 cáº£m biáº¿n

---

## ğŸ“ˆ Káº¾T QUáº¢ HIá»†U SUáº¤T

### 1ï¸âƒ£ Epoch Tá»‘t Nháº¥t (Epoch 15)
| Metric | Training | Validation |
|--------|----------|------------|
| **Accuracy** | 99.91% | 99.91% |
| **Loss** | 0.006002 | 0.006305 |

### 2ï¸âƒ£ Hiá»‡u Suáº¥t Cuá»‘i CÃ¹ng (Epoch 30)
| Metric | Training | Validation |
|--------|----------|------------|
| **Accuracy** | 99.99% | 99.91% |
| **Loss** | 0.000724 | 0.009400 |
| **Learning Rate** | 2.50e-05 | - |

### 3ï¸âƒ£ Káº¿t Quáº£ Kiá»ƒm Thá»­ Cuá»‘i CÃ¹ng
- **Test Accuracy:** 99.98%
- **Test Precision:** 0.9998
- **Test Recall:** 0.9998
- **Macro F1-Score:** 0.9991
- **Weighted F1-Score:** 0.9998

---

## ğŸ” PHÃ‚N TÃCH CHI TIáº¾T

### QuÃ¡ TrÃ¬nh Huáº¥n Luyá»‡n
- **Äá»™ chÃ­nh xÃ¡c ban Ä‘áº§u:** 41.30% (Epoch 1)
- **Äá»™ chÃ­nh xÃ¡c cuá»‘i:** 99.99% (Epoch 30)
- **Má»©c Ä‘á»™ cáº£i thiá»‡n:** 58.69%
- **Giáº£m Training Loss:** 99.91% (tá»« 0.846 xuá»‘ng 0.000724)
- **Giáº£m Validation Loss:** 95.40% (tá»« 0.204 xuá»‘ng 0.009400)

### Lá»‹ch TrÃ¬nh Learning Rate
MÃ´ hÃ¬nh sá»­ dá»¥ng **ReduceLROnPlateau** Ä‘á»ƒ Ä‘iá»u chá»‰nh learning rate:

1. **LR = 1.00e-04:** Epochs 1-22
   - Giai Ä‘oáº¡n há»c chÃ­nh, accuracy tÄƒng nhanh
   
2. **LR = 5.00e-05:** Epochs 23-29
   - Tinh chá»‰nh mÃ´ hÃ¬nh, giáº£m 50% learning rate
   
3. **LR = 2.50e-05:** Epoch 30
   - Giai Ä‘oáº¡n tinh chá»‰nh cuá»‘i, giáº£m thÃªm 50%

### PhÃ¢n TÃ­ch Overfitting
- **Gap cuá»‘i cÃ¹ng (Train-Val):** 0.08%
- **Gap trung bÃ¬nh (5 epochs cuá»‘i):** 0.07%
- **Tráº¡ng thÃ¡i:** âœ… **KHÃ”NG CÃ“ OVERFITTING NGHIÃŠM TRá»ŒNG**

MÃ´ hÃ¬nh tá»•ng quÃ¡t hÃ³a ráº¥t tá»‘t vá»›i gap giá»¯a training vÃ  validation accuracy < 0.1%, cho tháº¥y cÃ¡c ká»¹ thuáº­t regularization hoáº¡t Ä‘á»™ng hiá»‡u quáº£.

---

## âš™ï¸ Ká»¸ THUáº¬T Tá»I Æ¯U HÃ“A ÃP Dá»¤NG

MÃ´ hÃ¬nh sá»­ dá»¥ng **6 ká»¹ thuáº­t tá»‘i Æ°u hÃ³a** quan trá»ng:

### 1. Class Weights (Xá»­ lÃ½ máº¥t cÃ¢n báº±ng lá»›p)
- Tá»± Ä‘á»™ng tÃ­nh toÃ¡n trá»ng sá»‘ cho tá»«ng lá»›p
- Äáº£m báº£o mÃ´ hÃ¬nh chÃº Ã½ Ä‘áº¿n cÃ¡c lá»›p thiá»ƒu sá»‘

### 2. Learning Rate Scheduling (ReduceLROnPlateau)
- Giáº£m learning rate khi validation loss khÃ´ng cáº£i thiá»‡n
- Factor: 0.5
- Patience: 5 epochs

### 3. Early Stopping
- Ngá»«ng huáº¥n luyá»‡n khi khÃ´ng cÃ²n cáº£i thiá»‡n
- Patience: 15 epochs
- KhÃ´i phá»¥c weights tá»‘t nháº¥t

### 4. Dropout Regularization
- Dropout rates: 0.2 - 0.4
- NgÄƒn cháº·n overfitting
- Cáº£i thiá»‡n kháº£ nÄƒng tá»•ng quÃ¡t hÃ³a

### 5. Batch Normalization
- Chuáº©n hÃ³a activation giá»¯a cÃ¡c lá»›p
- á»”n Ä‘á»‹nh quÃ¡ trÃ¬nh huáº¥n luyá»‡n
- Cho phÃ©p learning rate cao hÆ¡n

### 6. Gradient Clipping
- Clipnorm: 1.0
- NgÄƒn cháº·n exploding gradients
- á»”n Ä‘á»‹nh quÃ¡ trÃ¬nh huáº¥n luyá»‡n LSTM

---

## ğŸ‰ ÄÃNH GIÃ Tá»”NG QUAN

### Äiá»ƒm Máº¡nh âœ…

1. **Hiá»‡u suáº¥t xuáº¥t sáº¯c**
   - Validation accuracy > 99%
   - Test accuracy Ä‘áº¡t 99.98%
   - Loss ráº¥t tháº¥p (< 0.01)

2. **KhÃ´ng overfitting**
   - Gap Train-Val < 0.1%
   - MÃ´ hÃ¬nh tá»•ng quÃ¡t hÃ³a tá»‘t
   - Hiá»‡u suáº¥t á»•n Ä‘á»‹nh trÃªn táº­p test

3. **Xá»­ lÃ½ tá»‘t class imbalance**
   - F1-score cao trÃªn táº¥t cáº£ cÃ¡c lá»›p
   - Precision vÃ  Recall cÃ¢n báº±ng
   - Class weights hiá»‡u quáº£

4. **QuÃ¡ trÃ¬nh huáº¥n luyá»‡n á»•n Ä‘á»‹nh**
   - Cáº£i thiá»‡n liÃªn tá»¥c qua cÃ¡c epochs
   - Learning rate schedule hoáº¡t Ä‘á»™ng tá»‘t
   - KhÃ´ng cÃ³ dáº¥u hiá»‡u gradient issues

### á»¨ng Dá»¥ng Thá»±c Táº¿ ğŸ­

MÃ´ hÃ¬nh nÃ y cÃ³ thá»ƒ Ä‘Æ°á»£c triá»ƒn khai Ä‘á»ƒ:

1. **Giáº£m thá»i gian ngá»«ng hoáº¡t Ä‘á»™ng** ğŸ’°
   - Dá»± Ä‘oÃ¡n há»ng hÃ³c trÆ°á»›c khi xáº£y ra
   - Láº­p káº¿ hoáº¡ch báº£o trÃ¬ chá»§ Ä‘á»™ng

2. **Tá»‘i Æ°u hÃ³a lá»‹ch báº£o trÃ¬** ğŸ”§
   - Báº£o trÃ¬ dá»±a trÃªn dá»± Ä‘oÃ¡n
   - Giáº£m chi phÃ­ báº£o trÃ¬ kháº©n cáº¥p

3. **Cáº£i thiá»‡n an toÃ n** âš ï¸
   - PhÃ¡t hiá»‡n sá»›m cÃ¡c báº¥t thÆ°á»ng
   - NgÄƒn ngá»«a sá»± cá»‘ nghiÃªm trá»ng

4. **Ra quyáº¿t Ä‘á»‹nh dá»±a trÃªn dá»¯ liá»‡u** ğŸ“Š
   - PhÃ¢n tÃ­ch xu hÆ°á»›ng há»ng hÃ³c
   - Tá»‘i Æ°u hÃ³a váº­n hÃ nh

---

## ğŸš€ HÆ¯á»šNG Cáº¢I TIáº¾N TÆ¯Æ NG LAI

### Cáº£i Thiá»‡n MÃ´ HÃ¬nh

1. **Bidirectional LSTM**
   - Há»c patterns theo cáº£ hai hÆ°á»›ng
   - CÃ³ thá»ƒ cáº£i thiá»‡n accuracy thÃªm 0.5-1%

2. **Attention Mechanism**
   - Táº­p trung vÃ o cÃ¡c timesteps quan trá»ng
   - Cáº£i thiá»‡n kháº£ nÄƒng diá»…n giáº£i

3. **Ensemble Methods**
   - Káº¿t há»£p nhiá»u mÃ´ hÃ¬nh
   - LSTM + GRU + CNN
   - TÄƒng Ä‘á»™ robust

### Cáº£i Thiá»‡n Dá»¯ Liá»‡u

4. **Thu tháº­p thÃªm dá»¯ liá»‡u**
   - Äáº·c biá»‡t cho lá»›p RECOVERING
   - Cáº£i thiá»‡n performance trÃªn minority class

5. **Data Augmentation**
   - SMOTE cho time-series
   - Táº¡o synthetic samples

6. **Feature Engineering**
   - ThÃªm cÃ¡c features thá»‘ng kÃª
   - Rolling averages, trends

### Triá»ƒn Khai

7. **Online Learning**
   - Há»c liÃªn tá»¥c tá»« dá»¯ liá»‡u má»›i
   - Cáº­p nháº­t mÃ´ hÃ¬nh Ä‘á»‹nh ká»³

8. **Model Monitoring**
   - Theo dÃµi performance trong production
   - Alert khi accuracy giáº£m

9. **Explainability**
   - Sá»­ dá»¥ng SHAP values
   - Giáº£i thÃ­ch predictions

---

## ğŸ“ Káº¾T LUáº¬N

### ThÃ nh Tá»±u ChÃ­nh

Dá»± Ã¡n Ä‘Ã£ hoÃ n thÃ nh thÃ nh cÃ´ng vá»›i cÃ¡c káº¿t quáº£ xuáº¥t sáº¯c:

âœ… **XÃ¢y dá»±ng mÃ´ hÃ¬nh LSTM hiá»‡u quáº£** cho dá»± Ä‘oÃ¡n há»ng hÃ³c mÃ¡y bÆ¡m  
âœ… **Xá»­ lÃ½ tá»‘t class imbalance** báº±ng nhiá»u ká»¹ thuáº­t  
âœ… **Ãp dá»¥ng 6 ká»¹ thuáº­t tá»‘i Æ°u hÃ³a** má»™t cÃ¡ch hiá»‡u quáº£  
âœ… **Äáº¡t accuracy 99.98%** trÃªn táº­p test  
âœ… **MÃ´ hÃ¬nh tá»•ng quÃ¡t hÃ³a tá»‘t** (khÃ´ng overfitting)  
âœ… **Sáºµn sÃ ng triá»ƒn khai** trong mÃ´i trÆ°á»ng thá»±c táº¿  

### BÃ i Há»c RÃºt Ra

1. **Tiá»n xá»­ lÃ½ dá»¯ liá»‡u** ráº¥t quan trá»ng cho time-series
2. **Class imbalance** cáº§n nhiá»u chiáº¿n lÆ°á»£c káº¿t há»£p
3. **Monitoring nhiá»u metrics** cho cÃ¡i nhÃ¬n toÃ n diá»‡n
4. **LSTM xuáº¥t sáº¯c** trong viá»‡c há»c temporal dependencies
5. **Optimization techniques** ngÄƒn cháº·n overfitting hiá»‡u quáº£

### TÃ¡c Äá»™ng Thá»±c Táº¿

MÃ´ hÃ¬nh nÃ y cÃ³ tiá»m nÄƒng:
- **Tiáº¿t kiá»‡m chi phÃ­** hÃ ng triá»‡u Ä‘Ã´ la tá»« downtime
- **Cáº£i thiá»‡n an toÃ n** cho cÃ´ng nhÃ¢n
- **Tá»‘i Æ°u hÃ³a váº­n hÃ nh** nhÃ  mÃ¡y
- **NÃ¢ng cao hiá»‡u quáº£** sáº£n xuáº¥t

---

## ğŸ“š TÃ€I LIá»†U THAM KHáº¢O

### Files ÄÃ£ LÆ°u
1. `best_pump_model.h5` - MÃ´ hÃ¬nh Ä‘Ã£ huáº¥n luyá»‡n
2. `scaler.pkl` - StandardScaler cho normalization
3. `label_encoder.pkl` - LabelEncoder cho labels
4. `training_history.json` - Lá»‹ch sá»­ huáº¥n luyá»‡n

### Sá»­ Dá»¥ng MÃ´ HÃ¬nh

```python
# Load model
from tensorflow import keras
import pickle

model = keras.models.load_model('best_pump_model.h5')
scaler = pickle.load(open('scaler.pkl', 'rb'))
label_encoder = pickle.load(open('label_encoder.pkl', 'rb'))

# Predict
predictions = model.predict(X_new)
predicted_classes = label_encoder.inverse_transform(predictions.argmax(axis=1))
```

---

**NgÃ y táº¡o:** December 15, 2025  
**Dá»± Ã¡n:** Industrial Pump Predictive Maintenance  
**KhÃ³a há»c:** 62FIT4ATI - Fall 2025  

---

*"Predictive maintenance is not just about preventing failures;  
it's about transforming how we think about industrial operations."*
